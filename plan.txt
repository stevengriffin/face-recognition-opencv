
input
@152x152x3 Pixels x RGB Frontalization
32x11x11x3 @142x142 Conv
ReLU
32x3x3x32 @71x71 Max-Pooling
ReLU
16x9x9x32 @63x63 Conv 
ReLU
16x9x9x16 @55x55 Locally-connected
ReLU
16x7x7x16 @25x25 Locally-connected
ReLU
16x5x5x16 @21x21 Locally-connected
ReLU
Fully-connected
ReLU
Dropout
Fully-connected (output)

================
Preliminary Setup

Open a terminal and clone the repository.

git clone TODO
cd TODO

sudo apt-get install python-pip
pip install virtualenvwrapper
export WORKON_HOME=~/.virtualenvs
mkdir -p $WORKON_HOME
source $(which virtualenvwrapper.sh)
mkvirtualenv face
pip install imutils numpy opencv-python torch Pillow scikit-learn progressbar2 amcrest

Whenever you open a new terminal to work on the project, you'll need to switch to your virtual environment by executing:

workon face

=================
Camera Setup

Power and connect the cameras with ethernet to the DGX station using POE injectors. 

Find the IP address, port, username, and password of the cameras. Learn to connect to the cameras, change their pan and tilt, save snapshots, and record streams with a Python script by reading https://python-amcrest.readthedocs.io/.

=================
Data Collection

Build a dataset of surveillance footage of yourselves. Record footage of each person with a variety of angles and lighting conditions. Organize it into a dataset with the following directory structure.

dataset/
    name1/
        00000.jpg
        00001.jpg
        00002.jpg
        00003.jpg
        ...
    name2/
        00000.jpg
        00001.jpg
        00002.jpg
        00003.jpg
        ...
    name3/
        00000.jpg
        00001.jpg
        00002.jpg
        00003.jpg
        ...

You can look at https://www.pyimagesearch.com/2018/06/11/how-to-build-a-custom-face-recognition-dataset/ for inspiration, but you should record footage from the 1080p Amcrest camera rather than a webcam.

==========

M1: face detector (pretrained neural network)

M2: face embeddings generator (pretrained neural network)

M3: face embeddings -> labels classifier (build yourself, many options: support vector machine, decision tree, k-nearest neighbors, neural network)

Generate face embeddings with OpenFace's neural network.
Run extract_embeddings.py --dataset path/to/your/dataset 

720p vs. 1080p
Record time to process results












Task 1: Use a pretrained network for generating face embeddings such as those provided in https://github.com/davidsandberg/facenet or https://github.com/cmusatyalab/openface and a pretrained network for detecting faces such as the ones used in OpenCV.


training pipeline:
for each image in the training dataset, detect face in image with M1 and get bounding box.
crop the image to the face and generate the face embeddings with M2. Add the face embedding to a list of vectors and the person's name to a list of labels. 
fit a machine learning model to the face embeddings and labels.

testing pipeline:
for each image in the testing dataset, detect face(s) in image with M1 and get their bounding boxes.
for each face, crop the image to the face, generate the face embeddings with M2, and match the face embeddings to a person's name with M3.
output a list of names of people detected in the image and the bounding boxes of their faces.
compute how many of the faces were predicted correctly by comparing the outputs to the labels

production pipeline:
when a new image arrives from the video cameras that might have a face, transfer it to the DGX and use M2 to find faces, if any exist, and if so, run M3 on the cropped image to recognize the names corresponding to the faces.

updating dataset: Each time a new person is added to the face recognition system, we need to run M1 and M2 on the new images then retrain M3.

==========

Develop a pipeline to integrate with the reception face database. 


